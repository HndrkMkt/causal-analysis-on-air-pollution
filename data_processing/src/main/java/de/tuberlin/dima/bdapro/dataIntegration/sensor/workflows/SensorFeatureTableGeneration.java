package de.tuberlin.dima.bdapro.dataIntegration.sensor.workflows;

import de.tuberlin.dima.bdapro.advancedProcessing.featureTable.Column;
import de.tuberlin.dima.bdapro.advancedProcessing.featureTable.BasicColumn;
import de.tuberlin.dima.bdapro.advancedProcessing.featureTable.FeatureTable;
import de.tuberlin.dima.bdapro.functions.TimeWindow;
import de.tuberlin.dima.bdapro.dataIntegration.sensor.UnifiedSensorReading;
import org.apache.commons.lang3.ArrayUtils;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.core.fs.Path;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.java.BatchTableEnvironment;
import org.apache.flink.table.sinks.CsvTableSink;
import org.apache.flink.table.sinks.TableSink;
import org.apache.flink.types.Row;

import java.util.ArrayList;
import java.util.List;

import static org.apache.flink.core.fs.FileSystem.WriteMode.OVERWRITE;


/**
 * Flink workflow that creates the feature table for the sensor data by filtering and aggregating the sensor data.
 *
 * @author Hendrik Makait
 */
public class SensorFeatureTableGeneration extends UnifiedSensorWorkflow {
    private static boolean compressed = false;

    private static final TypeInformation[] FIELD_TYPES = {Types.INT, Types.DOUBLE, Types.DOUBLE, Types.SQL_TIMESTAMP, Types.LONG,
            Types.LONG, Types.LONG, Types.BOOLEAN,
            Types.DOUBLE, Types.DOUBLE, Types.DOUBLE, Types.DOUBLE, Types.DOUBLE,
            Types.DOUBLE, Types.DOUBLE, Types.DOUBLE, Types.DOUBLE, Types.DOUBLE, Types.DOUBLE, Types.DOUBLE};

    /**
     * Aggregates the filtered dataset for all sensor data in "data_directory" into windows of 'window_size_in_minutes'
     * minutes and stores it in a combined csv file.
     */
    public static void main(String[] args) throws Exception {
        // set up the batch execution environment
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        final BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);

        ParameterTool params = ParameterTool.fromArgs(args);
        final String dataDirectory = params.get("data_dir", "data");
        final int windowInMinutes = params.getInt("window_in_minutes", 60);

        Table aggregates = aggregateSensorData(true, dataDirectory, windowInMinutes, env, tEnv);

        storeAggregatedSensorData(aggregates, dataDirectory, windowInMinutes, tEnv);
        env.execute("Filter Dataset");
    }

    /**
     * Generates a {@link FeatureTable} by aggregating filtered sensor data into windows of the specified size. The
     * filtered data is loaded from precalculated files.
     * <p>
     * Developer note: While this method uses stored data to avoid repeated processing of the large raw data, the
     * extension to using raw data is trivial.
     *
     * @param env                   An execution environment to use.
     * @param dataDirectory         The base path of the data directory.
     * @param windowInMinutes       The size of the aggregation window.
     * @param batchTableEnvironment A table environment to use.
     * @return the feature table generated by aggregating the filtered sensor data.
     */
    public static FeatureTable generateFeatureTable(ExecutionEnvironment env, String dataDirectory, int windowInMinutes, BatchTableEnvironment batchTableEnvironment) {
        Table aggregates = aggregateSensorData(true, dataDirectory, windowInMinutes, env, batchTableEnvironment);
        List<Column> columns = getFeatureColumns();
        List<Column> keyColumns = new ArrayList<>();
        String[] keyColumnNames = {"location", "timestamp"};
        for (String keyColumnName : keyColumnNames) {
            for (Column column : columns) {
                if (column.getName().equals(keyColumnName)) {
                    keyColumns.add(column);
                }
            }
        }
        return new FeatureTable("sensor", aggregates, columns, keyColumns, batchTableEnvironment);
    }

    /**
     * Stores the given aggregated data into a csv file.
     *
     * @param aggregates      The aggregated dataset to store.
     * @param dataDirectory   The base path of the data directory.
     * @param windowInMinutes The window size of the aggregated data used for file naming.
     * @param tEnv            The table environment to use.
     */
    public static void storeAggregatedSensorData(Table aggregates, String dataDirectory, int windowInMinutes, BatchTableEnvironment tEnv) {
        Path outputPath = new Path(dataDirectory, String.format("processed/output_%s.csv", windowInMinutes));
        TableSink<Row> sink = new CsvTableSink(outputPath.getPath(), ";", 1, OVERWRITE);

        tEnv.registerTableSink("output", getAggregationFieldNames(), getAggregationFieldTypes(), sink);
        aggregates.insertInto("output");
    }

    /**
     * Aggregates sensor data filtered by {@link SensorFiltering} by the given window size and returns the result.
     *
     * @param useCached       Whether or not to load prefiltered data from file or load raw data and apply filtering directly.
     * @param dataDirectory   The base path of the data directory.
     * @param windowInMinutes The size of the aggregation window in minutes.
     * @param env             An execution environment to use.
     * @param tEnv            A table environment to use.
     * @return the aggregated sensor data.
     */
    public static Table aggregateSensorData(boolean useCached, String dataDirectory, int windowInMinutes, ExecutionEnvironment env, BatchTableEnvironment tEnv) {
        DataSet<UnifiedSensorReading> sensorReadings = SensorFiltering.getFilteredSensors(useCached, dataDirectory, env);
        return aggregateSensorData(sensorReadings, windowInMinutes, env, tEnv);
    }

    /**
     * Aggregates the given sensor dataset by the given window size and returns the result.
     *
     * @param sensorReadings  The sensor data to aggregate.
     * @param windowInMinutes The size of the aggregation window in minutes.
     * @param env             An execution environment to use.
     * @param tEnv            A table environment to use.
     * @return the aggregated sensor data.
     */
    public static Table aggregateSensorData(DataSet<UnifiedSensorReading> sensorReadings, int windowInMinutes, ExecutionEnvironment env, BatchTableEnvironment tEnv) {
        tEnv.registerFunction("timeWindow", new TimeWindow(windowInMinutes));

        Table table = tEnv.fromDataSet(sensorReadings);

        StringBuilder selectStatement = new StringBuilder("location, MIN(lat) AS lat, MIN(lon) AS lon, " +
                "currentWindow AS `timestamp`, DAYOFYEAR(currentWindow) AS `dayOfYear`, " +
                "(HOUR(currentWindow) * 60) + MINUTE(currentWindow) AS `minuteOfDay`, DAYOFWEEK(currentWindow) AS `dayOfWeek`," +
                "(DAYOFWEEK(currentWindow) > 5) AS `isWeekend`");
        for (String field : UnifiedSensorReading.AGGREGATION_FIELDS) {
            selectStatement.append(String.format(", AVG(%s) AS %s", field, field));
        }
        Table aggregates = table.select("*, timeWindow(timestamp) AS currentWindow");
        return tEnv.sqlQuery("SELECT " + selectStatement.toString() + " FROM " + aggregates + " GROUP BY location, currentWindow");
    }

    /**
     * Returns the column metadata for the feature table to create.
     *
     * @return the column metadata for the feature table
     */
    private static List<Column> getFeatureColumns() {
        List<Column> featureColumns = new ArrayList<>();
        featureColumns.add(new BasicColumn("location", Types.INT, false));
        featureColumns.add(new BasicColumn("lat", Types.DOUBLE, true));
        featureColumns.add(new BasicColumn("lon", Types.DOUBLE, true));
        featureColumns.add(new BasicColumn("timestamp", Types.SQL_TIMESTAMP, true));
        featureColumns.add(new BasicColumn("dayOfYear", Types.LONG, true));
        featureColumns.add(new BasicColumn("minuteOfDay", Types.LONG, true));
        featureColumns.add(new BasicColumn("dayOfWeek", Types.LONG, true));
        featureColumns.add(new BasicColumn("isWeekend", Types.BOOLEAN, true));
        List<? extends Column> sensorFields = UnifiedSensorReading.getFields();
        for (Column column : sensorFields) {
            if (column.isFeature()) {
                featureColumns.add(column);
            }
        }
        return featureColumns;
    }

    /**
     * Returns names of the fields that are aggregated.
     *
     * @return the names of the fields that are aggregated
     */
    private static String[] getAggregationFieldNames() {
        String[] fieldNames = {"location", "lat", "lon", "timestamp", "dayOfYear", "minuteOfDay", "dayOfWeek", "isWeekend"};
        fieldNames = ArrayUtils.addAll(fieldNames, UnifiedSensorReading.AGGREGATION_FIELDS);
        return fieldNames;
    }

    /**
     * Returns the data types of the fields that are aggregated.
     *
     * @return the data types of the fields that are aggregated
     */
    private static TypeInformation[] getAggregationFieldTypes() {
        return FIELD_TYPES;
    }
}
